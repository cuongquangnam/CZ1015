{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "from sklearn import metrics\n",
    "import missingno as msn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "reg_data = pd.read_excel('WHR2018Chapter2OnlineData.xls',sheet_name = 'SupportingFactors')\n",
    "data = pd.read_excel('WHR2018Chapter2OnlineData.xls',sheet_name = 'Table2.1')\n",
    "region = pd.DataFrame(reg_data[[\"country\",\"Region indicator\"]])  # extracting region information from supporting factors sheet\n",
    "\n",
    "data_M = data.merge(region, on='country', how='left')         # inserting a column of region corresponding to countries to the dataset\n",
    "col = list(data_M)\n",
    "col.insert(1,col.pop(col.index('Region indicator')))\n",
    "data_M = data_M.loc[:,col]\n",
    "data_M['Region indicator'].fillna(\"None\", inplace=True)\n",
    "data_M.head(100)    # merged data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_M.describe() #Check the Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (16, 8))  #Distribution of Life Ladder of all countries in the data for given years\n",
    "sb.swarmplot(x=\"year\", y=\"Life Ladder\",  data=data_M, ax = axes)   #shown through swarmplot\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (16, 8))  #Distribution of Life Ladder of all countries in the data for given years\n",
    "sb.boxplot(x=\"year\", y=\"Life Ladder\",  data=data_M, ax = axes)   #shown throught boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10, 10))     #heatmap showing relationship of all the factors compared to one another\n",
    "sb.heatmap(data_M.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_data = pd.DataFrame(data_M[[\"Log GDP per capita\", \"Social support\", \"Healthy life expectancy at birth\",\n",
    "                                      \"Delivery Quality\"]])   #picking out factors with highest +ve correlation with Life ladder\n",
    "important_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(data = important_data)    #pair of those variables against one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = data_M.plot(kind='scatter', x=\"Log GDP per capita\", y='Life Ladder',alpha = 0.5,color = 'red',figsize=(12,9),subplots = (3,1,1))\n",
    "print(ax1)   #4 graphs showing each factor relating to life ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2 = data_M.plot(kind='scatter', x=\"Social support\", y='Life Ladder',alpha = 0.5,color = 'green',figsize=(12,9),subplots = (3,1,1))\n",
    "print(ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = data_M.plot(kind='scatter', x=\"Healthy life expectancy at birth\", y='Life Ladder',alpha = 0.5,color = 'purple',figsize=(12,9),subplots = (3,1,1))\n",
    "print(ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax4 = data_M.plot(kind='scatter', x=\"Delivery Quality\", y='Life Ladder',alpha = 0.5,color = 'black',figsize=(12,9),subplots = (3,1,1))\n",
    "print(ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_M['Region indicator'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list = list(data_M['Region indicator'].unique())    #average life ladder score for each region for all the given years combined\n",
    "region_happiness_score_average= []\n",
    "for i in region_list:\n",
    "    x = data_M[data_M['Region indicator'] == i]\n",
    "    region_happiness_score_rate = sum(x[\"Life Ladder\"])/len(x)\n",
    "    region_happiness_score_average.append(region_happiness_score_rate)\n",
    "\n",
    "data_bar = pd.DataFrame({'region_list':region_list, 'region_happiness_score_average':region_happiness_score_average})\n",
    "new_index = (data_bar['region_happiness_score_average'].sort_values(ascending = False)).index.values\n",
    "sorted_data = data_bar.reindex(new_index)\n",
    "\n",
    "#visualisation\n",
    "plt.figure(figsize=(10,7))\n",
    "sb.barplot(x=sorted_data['region_list'], y=sorted_data['region_happiness_score_average'])\n",
    "plt.xticks(rotation= 90)\n",
    "plt.xlabel('Regions')\n",
    "plt.ylabel('Happiness Score')\n",
    "plt.title('Average happiness score for regions between 2005-2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "happycountries = data_M['Life Ladder']>=6.5   #filtering out countries with relatively high life ladder\n",
    "happycountries = data_M[happycountries]\n",
    "happycountries[\"Region indicator\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = happycountries[\"Region indicator\"].value_counts().index\n",
    "colors = ['purple','blue','red','yellow','green','orange','lightcoral','pink','black']\n",
    "explode = [0,0,0,0,0,0,0,0,0]\n",
    "sizes = happycountries[\"Region indicator\"].value_counts().values\n",
    "\n",
    "# visual\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Distribution of the Happier Regions',color = 'blue',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middlecountries = data_M[5<=data_M['Life Ladder']]   #filtering out countries with mid life ladder score\n",
    "middlecountries = middlecountries[data_M['Life Ladder']<6.5]\n",
    "middlecountries[\"Region indicator\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = middlecountries[\"Region indicator\"].value_counts().index\n",
    "colors = ['purple','blue','red','yellow','green','orange','lightcoral','pink','brown','grey']\n",
    "explode = [0,0,0,0,0,0,0,0,0,0]\n",
    "sizes = middlecountries[\"Region indicator\"].value_counts().values\n",
    "\n",
    "# visual\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Distribution of the Mid-Happy Regions',color = 'blue',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadcountries = data_M['Life Ladder']<5   #filtering out countries with relatively low life ladder\n",
    "sadcountries = data_M[sadcountries]\n",
    "sadcountries[\"Region indicator\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sadcountries[\"Region indicator\"].value_counts().index\n",
    "colors = ['purple','blue','red','yellow','green','orange','lightcoral','pink','brown','grey']\n",
    "explode = [0,0,0,0,0,0,0,0,0,0]\n",
    "sizes = sadcountries[\"Region indicator\"].value_counts().values\n",
    "\n",
    "# visual\n",
    "plt.figure(figsize = (15,10))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Distribution of the Less Happier Regions',color = 'blue',fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_M.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_visualise the locations where the values are missing_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn.matrix(data_M.sample(1562))   # visualise the locations where the values are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_count the data points present for each variable in the dataset_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn.bar(data_M.sample(1562))    # counting the data points present for each variable in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling in the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_We are filling in the missing data for creating regression models, otherwise we would not have enough data points to be used as training data_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_scipy.interpolation was used to fill in the missing data values_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = data_M.interpolate(method = 'linear')    # use scipy.interpolation to fill in the missing data values\n",
    "data_pred.head(1562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_some data points are still missing beacause extrapolation was not done and those points lie outside the range of given data_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn.matrix(data_pred.sample(1562))    # some data points are still missing beacause extrapolation was not done and those points lie outside the range of given data\n",
    "\n",
    "msn.bar(data_pred.sample(1562))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_some data points are still missing beacause extrapolation was not done and those points lie outside the range of given data_**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of filled-in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_distribution of original data_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(data = data_M.drop(['year'],axis=1))    #distribution of original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_distribution of filled in data, which should resemble that of the original data_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(data = data_pred.drop(['year'],axis=1))    #distribution of filled in data, which should resemble that of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_M.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_statistical values of filled in data are plotted against those of the original data and the plots show that their statistical values have negligible variations_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "mean_ori=[]\n",
    "mean_pred=[]\n",
    "std_ori=[]\n",
    "std_pred=[]\n",
    "median_ori=[]\n",
    "median_pred=[]\n",
    "for var in data_M.drop(['country','year','Region indicator'], axis=1):\n",
    "    mean_ori.append(data_M[var].dropna().mean())\n",
    "    std_ori.append(data_M[var].dropna().std())\n",
    "    median_ori.append(data_M[var].dropna().median())\n",
    "\n",
    "for var in data_pred.drop(['country','year','Region indicator'],axis=1):\n",
    "    mean_pred.append(data_pred[var].dropna().mean())\n",
    "    std_pred.append(data_pred[var].dropna().std())\n",
    "    median_pred.append(data_pred[var].dropna().median())\n",
    "\n",
    "mean_o=pd.DataFrame(mean_ori)\n",
    "std_o=pd.DataFrame(std_ori)\n",
    "median_o=pd.DataFrame(median_ori)\n",
    "mean_p=pd.DataFrame(mean_pred)\n",
    "std_p=pd.DataFrame(std_pred)\n",
    "median_p=pd.DataFrame(median_pred)\n",
    "\n",
    "axes[0].scatter(mean_o, mean_p, color = \"red\")\n",
    "axes[0].plot(mean_o, mean_o, 'w-', linewidth = 1)\n",
    "axes[0].set_xlabel(\"Mean of original values\")\n",
    "axes[0].set_ylabel(\"Mean of filled values\")\n",
    "\n",
    "axes[1].scatter(std_o, std_p, color = \"green\")\n",
    "axes[1].plot(std_o, std_o, 'w-', linewidth = 1)\n",
    "axes[1].set_xlabel(\"std of original values\")\n",
    "axes[1].set_ylabel(\"std of filled values\")\n",
    "\n",
    "axes[2].scatter(median_o, median_p, color = \"blue\")\n",
    "axes[2].plot(median_o, median_o, 'w-', linewidth = 1)\n",
    "axes[2].set_xlabel(\"Median of original values\")\n",
    "axes[2].set_ylabel(\"Median of filled values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_complete = data_pred.dropna()    # we did not use extrapolation to find the missing data points outside the range of given data points because 1.their numbers are not great and 2.their values would be more unreliable as they will be predicted from predicted values.\n",
    "data_complete.head(1562)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_we did not use extrapolation to find the missing data points outside the range of given data points because of two reasons:\n",
    "    <br>\n",
    "    1.their numbers are not great and \n",
    "    <br>\n",
    "    2.their values would be more unreliable as they will be predicted from predicted values_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_complete.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_check the first column or first row to see which variable has the highest correlation coefficient with Life Ladder_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data_complete.drop(['year'],axis=1)\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 8))\n",
    "sb.heatmap(data_c.corr(), vmin = -1, vmax = 1, annot = True, fmt = \".2f\")    # check the first column or first row to see which variable has the highest correlation coefficient with Life Ladder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Preparation of train-test datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = data_complete.drop(['GINI index (World Bank estimate)'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_we are not going to use this column in building our model because the number data points given in the dataset was less than half of the number of the whole dataset and our model would be entirely based on our predicted values when we filled in the missing data points_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp = data_comp.drop(['Standard deviation of ladder by country-year','Standard deviation/Mean of ladder by country-year'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_we are not going to use these columns because they are directly related/calculated from life ladder, which is the variable that we are trying to predict_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp1 = data_comp.drop(['year','country','Region indicator'],axis = 1).reset_index()\n",
    "\n",
    "data_comp = data_comp1.drop(['index'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_we are not going to use these columns because they are string variables/do not have a linear relationship with life ladder_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data_comp['Life Ladder'])\n",
    "X = data_comp.drop(['Life Ladder'],axis = 1)    # drop life ladder becasue it is the variable we are trying to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_drop life ladder because it is the variable we are trying to predict_**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Linear regression model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print(\"X_Train Set :\", X_train.shape)\n",
    "print(\"y_Train Set :\", y_train.shape)\n",
    "print(\"X_Test Set  :\", X_test.shape)\n",
    "print(\"y_Test Set  :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linreg.fit(X_train, y_train)    # train the model with train data\n",
    "print('Intercept of Regression \\t: b = ', linreg.intercept_)\n",
    "print('Coefficients of Regression \\t: a = ', linreg.coef_)\n",
    "print()\n",
    "pd.DataFrame(list(zip(X_train.columns, linreg.coef_[0])), columns = [\"Predictors\", \"Coefficients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Using cross validation to get an estimate of how well the model will do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_L = cross_val_predict(linreg, X, y, cv=5)    # estimate how well this model will do for predictions\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y, y_val_pred_L, color = \"blue\")\n",
    "plt.plot(y, y, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Train)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\t\")\n",
    "print(\"Score of model (R^2) \\t:\", cross_val_score(linreg, X, y, cv=5))    # R^2 values for each training and validation iteration\n",
    "print(\"Error of prediction (MSE) \\t:\", mean_squared_error(y, y_val_pred_L))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y, y_val_pred_L))    # how well this model should do for predictions\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Actual performance of the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_L = linreg.predict(X_test)\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y_test, y_test_pred_L, color = \"green\")\n",
    "plt.plot(y_test, y_test, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Test)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Score of model (R^2) \\t:\", linreg.score(X_test, y_test))\n",
    "print(\"Erroe of prediction (MSE) \\t:\", mean_squared_error(y_test, y_test_pred_L))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y_test, y_test_pred_L))    # how well the model actually did on the test set\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Random forest regressor model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Using cross validation to get an estimate of how well the model will do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_val_pred_R = cross_val_predict(rfr, X, y.values.ravel(), cv=5)    # estimate how well this model will do for predictions\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y, y_val_pred_R, color = \"blue\")\n",
    "plt.plot(y, y, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Train)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\t\")\n",
    "print(\"Score of model (R^2) \\t:\", cross_val_score(rfr, X, y.values.ravel(), cv=5))    # R^2 values for each training and validation iteration\n",
    "print(\"Error of prediction (MSE) \\t:\", mean_squared_error(y, y_val_pred_R))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y, y_val_pred_R))    # how well this model should do for predictions\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Actual performance of the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test_pred_R = rfr.predict(X_test)\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y_test, y_test_pred_R, color = \"green\")\n",
    "plt.plot(y_test, y_test, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Test)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Test)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Score of model (R^2) \\t:\", rfr.score(X_test, y_test))\n",
    "print(\"Erroe of prediction (MSE) \\t:\", mean_squared_error(y_test, y_test_pred_R))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y_test, y_test_pred_R))    # how well the model actually did on the test set\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='purple'>Multi-layer perceptron regressior model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Data scaling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = pd.DataFrame(scaler.transform(X_train))\n",
    "X_te = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(y_train)\n",
    "y_tr = pd.DataFrame(scaler.transform(y_train))\n",
    "y_te = pd.DataFrame(scaler.transform(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**_Data scaling is important here to avoid exploding of gradient_**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpr = MLPRegressor(hidden_layer_sizes=(10), activation='tanh', solver='sgd',alpha=0.001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n",
    "               random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = mlpr.fit(X_tr,y_tr.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Using cross validation to get an estimate of how well the model will do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X)\n",
    "X_sc = pd.DataFrame(scaler.transform(X))\n",
    "scaler.fit(y)\n",
    "y_sc = pd.DataFrame(scaler.transform(y))\n",
    "\n",
    "y_val_pred_M = reg.predict(X_sc)\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y_sc, y_val_pred_M, color = \"blue\")\n",
    "plt.plot(y_sc, y_sc, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Train)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\t\")\n",
    "print(\"Score of model (R^2) \\t:\", cross_val_score(reg, X_sc, y_sc.values.ravel(), cv=5))    # R^2 values for each training and validation iteration\n",
    "print(\"Error of prediction (MSE) \\t:\", mean_squared_error(y_sc, y_val_pred_M))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y_sc, y_val_pred_M))    # how well this model should do for predictions\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='grey'> Actual performance of the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_M = reg.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "plt.scatter(y_te, y_test_pred_M, color = \"green\")\n",
    "plt.plot(y_te, y_te, 'w-', linewidth = 1)\n",
    "plt.xlabel(\"True values of the Response Variable (Train)\")\n",
    "plt.ylabel(\"Predicted values of the Response Variable (Train)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Goodness of Fit of Model \\tTest Dataset\")\n",
    "print(\"Score of model (R^2) \\t:\", reg.score(X_te, y_te))\n",
    "print(\"Erroe of prediction (MSE) \\t:\", mean_squared_error(y_te, y_test_pred_M))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y_te, y_test_pred_M))    # how well the model actually did on the test set\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of 2018 Life Ladder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2018 = pd.read_excel('WHR2019Chapter2OnlineData.xls',sheet_name = 'Table2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall = pd.DataFrame(data_2018[['Year','Country name','Life Ladder','Log GDP per capita','Social support','Healthy life expectancy at birth','Freedom to make life choices','Generosity','Perceptions of corruption','Positive affect','Negative affect','Confidence in national government','Democratic Quality','Delivery Quality','Standard deviation of ladder by country-year','Standard deviation/Mean of ladder by country-year','GINI index (World Bank estimate)','GINI index (World Bank estimate), average 2000-16','gini of household income reported in Gallup, by wp5-year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn.matrix(overall.sample(1704))\n",
    "msn.bar(overall.sample(1704))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2018 = overall.interpolate(method = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msn.matrix(pred_2018.sample(1704))\n",
    "msn.bar(pred_2018.sample(1704))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2018 = pred_2018.dropna()\n",
    "pred_2018 = pred_2018.drop(['GINI index (World Bank estimate)'],axis = 1)\n",
    "pred_2018 = pred_2018.drop(['Standard deviation of ladder by country-year','Standard deviation/Mean of ladder by country-year'],axis = 1)\n",
    "pred_2018 = pred_2018.drop(['Country name'],axis = 1)\n",
    "\n",
    "pred_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2018 = pred_2018[pred_2018[\"Year\"] == 2018]\n",
    "year_2018 = year_2018.drop(['Year'],axis = 1).reset_index()\n",
    "year_2018 = year_2018.drop(['index'],axis = 1)\n",
    "year_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_2018.head(135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2018 = pd.DataFrame(year_2018['Life Ladder'])\n",
    "X_2018 = year_2018.drop(['Life Ladder'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2018_pred_L = linreg.predict(X_2018)\n",
    "y_2018_pred_R = rfr.predict(X_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_2018)    # Data scaling for MLPRegressor\n",
    "X_M_2018 = pd.DataFrame(scaler.transform(X_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(y_2018)    # Data scaling for MLPRegressor\n",
    "y_M_2018 = pd.DataFrame(scaler.transform(y_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2018_pred_M = reg.predict(X_M_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
    "axes[0].scatter(y_2018, y_2018_pred_L, color = \"blue\")\n",
    "axes[0].plot(y_2018, y_2018, 'w-', linewidth = 1)\n",
    "axes[0].set_xlabel(\"True values of the Response Variable\")\n",
    "axes[0].set_ylabel(\"Predicted values of the Response Variable\")\n",
    "\n",
    "axes[1].scatter(y_2018, y_2018_pred_R, color = \"green\")\n",
    "axes[1].plot(y_2018, y_2018, 'w-', linewidth = 1)\n",
    "axes[1].set_xlabel(\"True values of the Response Variable\")\n",
    "axes[1].set_ylabel(\"Predicted values of the Response Variable\")\n",
    "\n",
    "axes[2].scatter(y_M_2018, y_2018_pred_M, color = \"red\")\n",
    "axes[2].plot(y_M_2018, y_M_2018, 'w-', linewidth = 1)\n",
    "axes[2].set_xlabel(\"True values of the Response Variable\")\n",
    "axes[2].set_ylabel(\"Predicted values of the Response Variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Goodness of Fit of Linear Regression Model \\t\")\n",
    "print(\"Score of the model (R^2) \\t:\", linreg.score(X_2018, y_2018))\n",
    "print(\"Error of prediction (MSE) \\t:\", mean_squared_error(y_2018, y_2018_pred_L))\n",
    "print(\"Accuracy of prediction:\", metrics.r2_score(y_2018, y_2018_pred_L))\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Random Forest Regressor Model \\t\")\n",
    "print(\"Score of model (R^2) \\t:\", rfr.score(X_2018, y_2018))\n",
    "print(\"Erroe of prediction (MSE) \\t:\", mean_squared_error(y_2018, y_2018_pred_R))\n",
    "print(\"Accuracy of prediction \\t:\", metrics.r2_score(y_2018, y_2018_pred_R))\n",
    "print()\n",
    "\n",
    "print(\"Goodness of Fit of Linear Regression Model \\t\")\n",
    "print(\"Score of the model (R^2) \\t:\", reg.score(X_M_2018, y_M_2018))\n",
    "print(\"Error of prediction (MSE) \\t:\", mean_squared_error(y_M_2018, y_2018_pred_M))\n",
    "print(\"Accuracy of prediction:\", metrics.r2_score(y_M_2018, y_2018_pred_M))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores obtained from cross validation and those from the prediction for the test datasets, MLPRegressor and random forest regressor have higher scores and accuracies than the linear regression model, and hence would be the better models for Life Ladder predictions in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2017 = data_complete[data_complete['year']==2017]\n",
    "cluster_data2017 = pd.DataFrame(data_2017[['Life Ladder','Log GDP per capita','Social support','Healthy life expectancy at birth','Freedom to make life choices','Perceptions of corruption','Confidence in national government','Democratic Quality','Delivery Quality']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.pairplot(cluster_data2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_clust = 1\n",
    "max_clust = 40\n",
    "init_algo = 'k-means++'\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# Computer Within Cluster Sum of Squares\n",
    "within_ss=[]\n",
    "for num_clust in range(min_clust,max_clust+1):\n",
    "    kmeans = KMeans(n_clusters = num_clust, init = init_algo,n_init=5)\n",
    "    kmeans.fit(cluster_data2017)\n",
    "    within_ss.append(kmeans.inertia_)\n",
    "\n",
    "# Angle Plot: Within SS vs Number of Clusters\n",
    "f, axes = plt.subplots(1,1,figsize=(16,4))\n",
    "plt.plot(range(min_clust,max_clust+1),within_ss)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within Cluster sum of Squares')\n",
    "plt.xticks(np.arange(min_clust, max_clust+1,1.0))\n",
    "plt.grid(which='major', axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clust = 4\n",
    "init_algo = \"k-means++\"\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clust, init = init_algo, n_init=20)\n",
    "kmeans.fit(cluster_data2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.predict(cluster_data2017)\n",
    "cluster_labeled = cluster_data2017.copy()\n",
    "cluster_labeled[\"Cluster\"] = pd.Categorical(labels)\n",
    "sb.countplot(cluster_labeled[\"Cluster\"])\n",
    "cluster_labeled[cluster_labeled['Cluster']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(cluster_labeled, vars = cluster_data2017.columns.values, hue = \"Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['Life Ladder','Log GDP per capita','Social support','Healthy life expectancy at birth','Freedom to make life choices','Perceptions of corruption','Confidence in national government','Democratic Quality','Delivery Quality']\n",
    "count = 0\n",
    "f, axes = plt.subplots(9,1, figsize=(16,36))\n",
    "for i in a:\n",
    "    sb.boxplot(x = i,y='Cluster',data=cluster_labeled,ax =axes[count])\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster 1 has the highest Life Ladder, GDP per capita, Social support and healthy life expectancy at birth.\n",
    "Cluster 2 has the lowest Healthy life expectancy at birth and Log GDP per capita\n",
    "Cluster 0 and 3 are an intermediate clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence on Happiness Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of regions:\",len(data_complete['Region indicator'].unique())-1)    # len()-1 because \"none\" is not a region\n",
    "data_complete['Region indicator'].value_counts()    # information about the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 1, figsize=(32, 5))\n",
    "sb.countplot(data_complete[\"Region indicator\"])\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "highest_influence = pd.DataFrame(columns = ['Region','Most important factor','Point'])\n",
    "region = data_complete['Region indicator'].unique()\n",
    "for r in region:\n",
    "    a = pd.DataFrame(pd.DataFrame(data_complete[data_complete[\"Region indicator\"]==r]).corr()['Life Ladder'].nlargest())\n",
    "    highest_influence = highest_influence.append({'Region':r,'Most important factor':a.index[1],'Point':a.loc[a.index[1],'Life Ladder']},ignore_index=True)\n",
    "    #print(a.index[1], a.loc[a.index[1],'Life Ladder'])\n",
    "print(highest_influence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
